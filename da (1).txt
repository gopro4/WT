slip1_2  )Create ‘Position_Salaries’ Data set. Build a linear regression model by identifying independent and
target variable. Split the variables into training and testing sets. then divide the training and testing sets
into a 7:3 ratio, respectively and print them. Build a simple linear regression model.



import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Step 1: Generate the dataset
positions = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)  # Independent variable: Position
salaries = np.array([45000, 50000, 60000, 80000, 110000, 150000, 200000, 300000, 500000, 1000000])  # Target variable: Salary

# Step 2: Identify independent and target variable
X = positions
y = salaries

# Step 3: Split dataset into training and testing sets (7:3 ratio)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 4: Print the training and testing sets
print("Training set:")
print("X_train:")
print(X_train)
print("y_train:")
print(y_train)
print("\nTesting set:")
print("X_test:")
print(X_test)
print("y_test:")
print(y_test)

# Step 5: Build a simple linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Print coefficients
print("\nIntercept:", model.intercept_)
print("Coefficient:", model.coef_[0])

____________________________________________________________________________________________________________________________________________________________________________________________________


slip_5_2Use the iris dataset. Write a Python program to view some basic statistical details like percentile,
mean, std etc. of the species of 'Iris-setosa', 'Iris-versicolor' and 'Iris-virginica'. Apply logistic regression
on the dataset to identify different species (setosa, versicolor, verginica) of Iris flowers given just 4
features: sepal and petal lengths and widths.. Find the accuracy of the model. 


import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# Load Iris dataset
iris = load_iris()
iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],
                       columns=iris['feature_names'] + ['species'])

# Basic statistical details of each species
setosa_stats = iris_df[iris_df['species'] == 0].describe()
versicolor_stats = iris_df[iris_df['species'] == 1].describe()
virginica_stats = iris_df[iris_df['species'] == 2].describe()

print("Statistics for Iris-setosa:")
print(setosa_stats)
print("\nStatistics for Iris-versicolor:")
print(versicolor_stats)
print("\nStatistics for Iris-virginica:")
print(virginica_stats)

# Prepare data for logistic regression
X = iris_df.drop('species', axis=1)
y = iris_df['species']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply logistic regression
logistic_model = LogisticRegression(max_iter=1000)
logistic_model.fit(X_train, y_train)

# Predictions
y_pred = logistic_model.predict(X_test)

# Accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print("\nAccuracy of the logistic regression model:", accuracy)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix
print("\nConfusion Matrix:")
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', fmt='g')


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



slip6_2  )Create the following dataset in python & Convert the categorical values into numeric format.Apply
the apriori algorithm on the above dataset to generate the frequent itemsets and association rules. Repeat
the process with different min_sup values. 

from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import pandas as pd

# Create the dataset
dataset = [
    ['milk', 'bread'],
    ['bread', 'diaper', 'beer', 'egg'],
    ['milk', 'beer', 'diaper', 'coke'],
    ['bread', 'milk', 'diaper', 'beer'],
    ['bread', 'milk', 'diaper', 'coke']
]

# Convert categorical values into numeric format
te = TransactionEncoder()
te_ary = te.fit(dataset).transform(dataset)
df = pd.DataFrame(te_ary, columns=te.columns_)

# Apply Apriori algorithm with different min_sup values
min_sup_values = [0.2, 0.4, 0.6]  # Adjust as needed
for min_sup in min_sup_values:
    frequent_itemsets = apriori(df, min_support=min_sup, use_colnames=True)
    print("\nFrequent itemsets with min_sup =", min_sup)
    print(frequent_itemsets)

    # Generate association rules
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)
    print("\nAssociation rules with min_sup =", min_sup)
    print(rules)

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


slip7_2   )Download the Market basket dataset. Write a python program to read the dataset and display its
information. Preprocess the data (drop null values etc.) Convert the categorical values into numeric
format. Apply the apriori algorithm on the above dataset to generate the frequent itemsets and association
rules. 

import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

# Step 1: Download the Market basket dataset and save it as 'market_basket.csv'
# Step 2: Read the dataset and display its information
market_basket_data = pd.read_csv('market_basket.csv')

print("Information about the Market Basket dataset:")
print(market_basket_data.info())

# Step 3: Preprocess the data (drop null values, if any)
market_basket_data.dropna(inplace=True)

# Step 4: Convert categorical values into numeric format using one-hot encoding
transaction_encoder = TransactionEncoder()
market_basket_encoded = transaction_encoder.fit_transform(market_basket_data.values)
market_basket_df = pd.DataFrame(market_basket_encoded, columns=transaction_encoder.columns_)

# Step 5: Apply the Apriori algorithm to generate frequent itemsets and association rules
frequent_itemsets = apriori(market_basket_df, min_support=0.05, use_colnames=True)
association_rules_df = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

print("\nFrequent itemsets:")
print(frequent_itemsets)

print("\nAssociation rules:")
print(association_rules_df)


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


slip9_2   Create your own transactions dataset and apply the above process on your dataset.

import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

# Create a sample transactions dataset
transactions = [
    ['milk', 'bread', 'eggs'],
    ['bread', 'butter', 'jam', 'eggs'],
    ['milk', 'bread', 'butter', 'cheese'],
    ['bread', 'jam'],
    ['milk', 'eggs', 'cheese']
]

# Convert transactions into a DataFrame
df = pd.DataFrame(transactions)

# Convert categorical values into numeric format using one-hot encoding
transaction_encoder = TransactionEncoder()
transaction_encoded = transaction_encoder.fit_transform(df)
df_encoded = pd.DataFrame(transaction_encoded, columns=transaction_encoder.columns_)

# Apply Apriori algorithm to generate frequent itemsets
frequent_itemsets = apriori(df_encoded, min_support=0.2, use_colnames=True)

# Generate association rules
association_rules_df = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)

# Display frequent itemsets and association rules
print("Frequent Itemsets:")
print(frequent_itemsets)

print("\nAssociation Rules:")
print(association_rules_df)

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



slip 17_2    Consider text paragraph.So, keep working. Keep striving. Never give up. Fall down seven times, get
up eight. Ease is a greater threat to progress than hardship. Ease is a greater threat to progress than
hardship. So, keep moving, keep growing, keep learning. See you at work.Preprocess the text to remove
any special characters and digits. Generate the summary using extractive summarization process


import re
import nltk
from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans

# Sample text paragraph
text = """Keep working. Keep striving. Never give up. Fall down seven times, get up eight. 
Ease is a greater threat to progress than hardship. Ease is a greater threat to progress than hardship. 
So, keep moving, keep growing, keep learning. See you at work."""

# Preprocess the text to remove special characters and digits
preprocessed_text = re.sub(r'[^a-zA-Z\s]', '', text)

# Tokenize the text into sentences
sentences = sent_tokenize(preprocessed_text)

# Compute TF-IDF matrix
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)

# Calculate similarity matrix based on cosine similarity
similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Apply KMeans clustering
num_clusters = int(len(sentences) / 2)  # Adjust as needed
kmeans = KMeans(n_clusters=num_clusters)
kmeans.fit(similarity_matrix)

# Generate summary by selecting representative sentences from each cluster
summary = []
for cluster_id in range(num_clusters):
    cluster_sentences = [sentences[i] for i, label in enumerate(kmeans.labels_) if label == cluster_id]
    representative_sentence = max(cluster_sentences, key=len)  # Select the longest sentence
    summary.append(representative_sentence)

# Join the summary sentences into a single string
summary_text = ' '.join(summary)

# Print the summary
print("Summary:")
print(summary_text)


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


slip 19_2   Download the movie_review.csv dataset from Kaggle by using the following link
:https://www.kaggle.com/nltkdata/movie-review/version/3?select=movie_review.csv to perform
sentiment analysis on above dataset and create a wordcloud

import pandas as pd
from textblob import TextBlob
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Step 1: Read the dataset into a DataFrame
movie_review_data = pd.read_csv('movie_review.csv')

# Step 2: Perform sentiment analysis
# Assuming 'review' column contains the movie reviews
movie_review_data['polarity'] = movie_review_data['review'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)

# Step 3: Create a wordcloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(movie_review_data['review']))

# Plot the wordcloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


slip20_2     )Consider text paragraph."""Hello all, Welcome to Python Programming Academy. Python
Programming Academy is a nice platform to learn new programming skills. It is difficult to get enrolled
in this Academy."""Remove the stopwords.


import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download NLTK stopwords if not already downloaded
nltk.download('stopwords')
nltk.download('punkt')

# Given text paragraph
text = """Hello all, Welcome to Python Programming Academy. Python Programming Academy is a nice platform to learn new programming skills. It is difficult to get enrolled in this Academy."""

# Tokenize the text into words
words = word_tokenize(text)

# Get English stopwords
stop_words = set(stopwords.words('english'))

# Remove stopwords from the text
filtered_words = [word for word in words if word.lower() not in stop_words]

# Join the filtered words into a single string
filtered_text = ' '.join(filtered_words)

print("Text after removing stopwords:")
print(filtered_text)



----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


slip 21_2     Build a simple linear regression model for User Data. 


import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Generate some sample user data (age and score)
np.random.seed(0)
age = np.random.randint(20, 60, 100)  # Generate 100 random ages between 20 and 60
score = 50 + 0.5 * age + np.random.normal(0, 5, 100)  # Generate scores based on age with some noise

# Reshape the data
age = age.reshape(-1, 1)  # Reshape to a column vector

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(age, score, test_size=0.2, random_state=42)

# Build the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Plot the data points and the regression line
plt.scatter(X_test, y_test, color='blue', label='Actual')
plt.plot(X_test, y_pred, color='red', label='Predicted')
plt.xlabel('Age')
plt.ylabel('Score')
plt.title('Linear Regression Model for User Data')
plt.legend()
plt.show()

# Print the coefficients
print("Intercept:", model.intercept_)
print("Coefficient:", model.coef_[0])


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


slip30_2    Create the dataset . transactions = [['eggs', 'milk','bread'], ['eggs', 'apple'], ['milk', 'bread'], ['apple',
'milk'], ['milk', 'apple', 'bread']] .
Convert the categorical values into numeric format.Apply the apriori algorithm on the above dataset to
generate the frequent itemsets and association rules. 


from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

# Given transactions dataset
transactions = [
    ['eggs', 'milk', 'bread'],
    ['eggs', 'apple'],
    ['milk', 'bread'],
    ['apple', 'milk'],
    ['milk', 'apple', 'bread']
]

# Convert categorical values into numeric format using TransactionEncoder
encoder = TransactionEncoder()
trans_array = encoder.fit_transform(transactions)
trans_df = pd.DataFrame(trans_array, columns=encoder.columns_)

# Apply Apriori algorithm to generate frequent itemsets
frequent_itemsets = apriori(trans_df, min_support=0.2, use_colnames=True)

# Generate association rules
association_rules_df = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)

print("Frequent Itemsets:")
print(frequent_itemsets)
print("\nAssociation Rules:")
print(association_rules_df)
